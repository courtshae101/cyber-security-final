---
title: "State_and_indiv"
author: "Courtney"
date: "4/5/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)
library(ggpubr)
```


```{r}
breaches <- read_csv('C:/Users/student/Documents/SYS 2202/cyber-security-final/courtney-final-variables/Cyber Security Breaches (1).csv',
                     col_types = cols(
                      State = col_factor(),
                      Individuals_Affected = col_integer()
                     )
                  )
```
```{r}
head(breaches)
```

### 3.1.3 Variable 3  
State

#### 3.1.2.1 Visualising distributions (Barcharts, Histograms) (5 points)

```{r}
state_bar <- breaches %>%
  mutate(State = State %>% fct_infreq() %>% fct_rev()) %>%
  ggplot(aes(x=State)) +
  geom_bar()+
  coord_flip()

state_bar

count_state <- breaches %>%
  mutate(State = State %>% fct_infreq() %>% fct_rev()) %>%
  count(State)

count_state
```


**- Which values are the most common? Why?** 

Breaches in the State of California are the most common since they have the most breaches at 113. This is most likely due to the fact that California is highly populated with lots of buisnesses and tech industries, therefore can have more opportunities for breaches.      

**- Which values are rare? Why? Does that match your expectations?** 
The most rare values are VT, ME, SD, and HI which all have only one breach.  Since these are not very largely populated states this does make sense. 
 

**- Can you see any unusual patterns? What might explain them?**

There does not appear to be any unusal patterns in the State breach count.  Some states have more breaches than others but there is not any outliers of cycles of number of breaches.  

**- Are there clusters in the data? If so,**
No there are no clusters in the data, all of the data is relatively evenly distributed.

  **- How are the observations within each cluster similar to or different from each other?**
  
  As mentioned above there are no clusters present.  
  
  **- How can you explain or describe the clusters?**

  As mentioned above there are no clusters present.
  
#### 3.1.2.2 Unusual values (2 points) 

**- Describe and demonstrate how you determine if there are unusual values in the data. E.g. too large, too small, negative, etc.**

```{r}
breaches %>%
  mutate(State = State %>% fct_infreq() %>% fct_rev()) %>%
  ggplot(aes(x=State)) +
  geom_bar()+
  coord_flip()
```

There were no negative state breaches and no values that were unexpectadly high or low.  This is seen in the bar graph.  More exploration has to be done to determine if any values should be removed.

**- Describe and demonstrate how you determine if they are outliers.**

An outlier is 1.5 times the interquartile range away from either the lower or upper quartile.  In order to determine if any of the state count values are outliers the interquartile range, first quartile, and third quartile need to be calculated.  The State count data then has to be filtered for values that are less than the first quartile minus the IQR times 1.5 and values that are greater than the third quartile plus the IQR times 1.5. The outliers can be seen in the outlier list data frame, it includes, TX, CA, FL.  


```{r}
state_count <- breaches %>%
  group_by(State) %>%
  count()
state_count
stdev <-  sd(state_count$n, na.rm = TRUE)
stdev

innerQ <-  IQR(state_count$n, na.rm = TRUE)
innerQ

firstQ <- quantile(state_count$n, 0.25, na.rm = TRUE)
firstQ <- firstQ[[1]]

thirdQ <- quantile(state_count$n, 0.75, na.rm = TRUE)
thirdQ <- thirdQ[[1]]

outlier_list <- state_count %>%
  filter(n < (firstQ - innerQ * 1.5) | 
        n > (thirdQ + innerQ * 1.5))

outlier_list

```

**- Show how do your distributions look like with and without the unusual values.**

 With the outliers removed the distribution is made narrower with less variation. Since the largest state breach counts are removed overall the distribution becomes more similar throughout. 

```{r}
outlier_state = c("TX", "CA", "FL")

no_out_bar <- breaches %>%
  mutate(State = State %>% fct_infreq() %>% fct_rev()) %>%
  filter(!(State %in% outlier_state)) %>%
  ggplot(aes(x=State)) +
  geom_bar()+
  coord_flip()+
  ylim(0, 110) +
  labs(title = "Outliers removed")

out_in_bar <- breaches %>%
  mutate(State = State %>% fct_infreq() %>% fct_rev()) %>%
  ggplot(aes(x=State)) +
  geom_bar()+
  coord_flip()+
  ylim(0,110) +
  labs(title = "Outliers included")

ggarrange(no_out_bar, out_in_bar, ncol = 2)

```

**- Discuss whether or not you need to remove unusual values and why.**

Since the largest values will provide d the most insight into why breaches are happeing at such a large rate in certain states they should not be removed. 

#### 3.1.2.3 Missing values (2 points)

**- Does this variable include missing values? Demonstrate how you determine that.**

There are no missing values in the State variable.  The method is.na with the column name can be used and then the vector returned can be turned into a data frame that represents the number of NA values (TRUE) and non NA values (FALSE).  It can also be confirmed by calling summary() on the State, which also shows that there are no NA values in the State variable.  There should also be information for all 50 states plus PR and DC, which is confirmed using unique() to show there are 52 unqique State values.   

```{r}
missing <- is.na(breaches$State)

num_missing <- as.data.frame(table(missing))

num_missing
  
summary(breaches$State)

breaches$State %>%
  unique()
```

**- Demonstrate and discuss how you handle the missing values. E.g., removing, replacing with a constant value, or a value based on the distribution, etc.**

There are no missing values so they do no need to be handled. 


**- Show how your data looks in each case after handling missing values.Describe and discuss the distribution.** 

Since there is no missing values the distribution does not changed, see earlier bar graph for distribution.    
 

#### 3.1.2.4 Does converting the type of this variable help exploring the distribution of its values or identifying outliers or missing values? (3)

Yes converting State to a logical may be helpful in exploring the distribution of its values or identifying outliers or missing values since logical are simpler to evaluate when larger continuous data is converted into two groups.  

**- What type can the variable be converted to?**

State is of type factor, but it can converted to a logical.  By making the value of State TRUE when the State is in the northeast and FALSE when the value of the State is not in the northeast, we can see if the northeast has a large number of breaches.  Converting State to a logical is a simpler way to interpret State values. The converted State type is saved as a new variable northeast.

```{r}
northeast_list <- c("CT", "MA", "NH", "NJ", "NY", "PA", "RI", "VT", "DE", "MD")
#function to determine if the states are in the northeast
northeast_check <- function(x) {
  if(is.na(x)){
    return(NA)
  }
  else if(x %in% northeast_list){
    return(TRUE)
  }
  else{
    return(FALSE)
  }
}

breaches$northeast <- sapply(breaches$State, northeast_check)
head(breaches)
```


**- How will the distribution look? Please demonstrate with appropriate plots.**

From plotting the converted logical State as a bar graph, we can see that the majority of the breaches were not in the northeast. However the number of breaches is large for the northeast since there is only 9 states vs the other 43 States and territories.  We can also see that there are no NA values, which confirms the analysis done earlier. 

```{r}
breaches %>%
  ggplot(aes(x=northeast, fill = northeast)) +
  geom_bar()
```

#### 3.1.2.5 What new variables do you need to create?  (3)


**- List the variables**
northeast, westcoast, midwest, south.

All are logical variables that are true or false if the breach is in the region. 

```{r}

westcoast_list <- c("WY", "CO", "UT", "NV", "ID", "CA", "OR", "WA", "AK")
#function to determine if the states are on the West Coast
westcoast_check <- function(x) {
  if(is.na(x)){
    return(NA)
  }
  else if(x %in% westcoast_list){
    return(TRUE)
  }
  else{
    return(FALSE)
  }
}

breaches$westcoast <- sapply(breaches$State, westcoast_check)
head(breaches)

```

```{r}
midwest_list <- c("ND", "SD", "NE", "KS", "MO", "IA", "MN", "WI", "MI", "IL", "IN", "OH")
#function to determine if the states are in the midwest
midwest_check <- function(x) {
  if(is.na(x)){
    return(NA)
  }
  else if(x %in% midwest_list){
    return(TRUE)
  }
  else{
    return(FALSE)
  }
}

breaches$midwest <- sapply(breaches$State, midwest_check)
head(breaches)
```

```{r}
south_list <- c("MD", "DE", "VA", "WV", "KY", "TN", "NC", "SC", "FL", "GA", "AL", "MS", "LA", "AK", "TX", "OK", "DC", "PR")
#function to determine if the states are in the south
south_check <- function(x) {
  if(is.na(x)){
    return(NA)
  }
  else if(x %in% south_list){
    return(TRUE)
  }
  else{
    return(FALSE)
  }
}

breaches$south <- sapply(breaches$State, south_check)
head(breaches)

```

**- Describe and discuss why they are needed and how you plan to use them.**
northeast, westcoast, midwest, and south, are all a logical variable.  They are needed in exploring the distribution of breaches per state in different regions of the US.  Logical variables are used since logical are simpler to evaluate when larger factor data is converted into two groups.  I plan to use the variables to compare individuals affected by their location. 



```{r}
northeast_bar <- 
breaches %>%
  ggplot(aes(x=northeast, fill = northeast)) +
  geom_bar()

midwest_bar <- 
breaches %>%
  ggplot(aes(x=midwest, fill = midwest)) +
  geom_bar()

westcoast_bar <- 
breaches %>%
  ggplot(aes(x=westcoast, fill = westcoast)) +
  geom_bar()

south_bar <- 
breaches %>%
  ggplot(aes(x=south, fill = south)) +
  geom_bar()

ggarrange(northeast_bar, midwest_bar, westcoast_bar, south_bar, nrow = 2, ncol = 2)
```
### 3.1.3 Variable 3  
Individuals_affected

#### 3.1.2.1 Visualising distributions (Barcharts, Histograms) (5 points)

```{r}
indiv_box <- breaches %>%
  ggplot(aes(x=Individuals_Affected)) +
  geom_boxplot()

indiv_hist <- breaches %>%
  ggplot(aes(x=Individuals_Affected)) +
  geom_histogram()

indiv_box_zoom <- breaches %>%
  ggplot(aes(x=Individuals_Affected)) +
  geom_boxplot()+
  xlim(0, 35000) + 
  labs(title = "0 to 35,000 zoom in")

indiv_hist_zoom <- breaches %>%
  ggplot(aes(x=Individuals_Affected)) +
  geom_histogram() + 
  xlim(0, 35000) + 
  labs(title = "0 to 35,000 zoom in")

ggarrange(indiv_box, indiv_hist, indiv_box_zoom, indiv_hist_zoom,  nrow = 2, ncol = 2)

summary(breaches)

IQR(breaches$Individuals_Affected, na.rm = TRUE)
```


**- Which values are the most common? Why?** 

The values in the IQR are the most common which ranges from 500 to 6941 people. This can be seen in the histogram since the peak is centered around 2300 people, which is the median. The majority of the values fall in this range and therefore they are statistically the most common.  This can be interpreted that in most data breaches the number of individuals affected is usually between 500 to around 7000 people.    

**- Which values are rare? Why? Does that match your expectations?** 

Wind levels that are above 473.3 mph (3rd Quartile + 1.5 * IQR) or below -98.7 mph (1st Quartile - 1.5 * IQR) are rare since they are outliers in the data.  There can only be solar radiation of 0 however so the lower outlier bound is not applicable.  There are no outliers in this data and therefore it can be derived that there are no "rare" values.  Overall the data for solar radiation is very evenly distributed and therefore rare values will not exist.  This does make sense since the sun is shinning everyday and the slight variation has to do with weather which results in most solar radiation values being common. 

```{r}
airquality %>%
  ggplot(aes(x=Solar.R)) +
  geom_boxplot()

#no outliers
```


**- Can you see any unusual patterns? What might explain them?**

There is no cycle pattern present in the individuals affected data.  The only slightly unusual pattern is that there is a strong right skew.  There are some very large values for indivduals affected that drag the mean up, and therefore the data is very right skewed.  Overall the median is a better reference to the middle of the data than the mean. This right skew is caused by a few data breaches that had very high numbers of indivuals affected. 

**- Are there clusters in the data? If so,**
No there are no clusters in the data, but as mentioned above there is a right skew. 

  **- How are the observations within each cluster similar to or different from each other?**
  
  As mentioned above there are no clusters present.  
  
  **- How can you explain or describe the clusters?**

  As mentioned above there are no clusters present.
  
#### 3.1.2.2 Unusual values (2 points) 

**- Describe and demonstrate how you determine if there are unusual values in the data. E.g. too large, too small, negative, etc.**

There are no negative values for individuals affected, and there are two very large values, above 3 million.  I filtered for both situations to confirm this result of ununusal values. 

```{r}
neg_indiv <- breaches %>%
  filter(Individuals_Affected < 0)

neg_indiv

large_indiv <- breaches %>%
  filter(Individuals_Affected > 3000000)

large_indiv
```


**- Describe and demonstrate how you determine if they are outliers.**

An outlier is 1.5 times the interquartile range away from either the lower or upper quartile.  In order to determine if any of the indivuals affected values are outliers the interquartile range, first quartile, and third quartile need to be calculated.  The indivduals affected data then has to be filtered for values that are less than the first quartile minus the IQR times 1.5 and values that are greater than the third quartile plus the IQR times 1.5. The 129 outliers can be seen in the outlier list. 
```{r}
stdev <-  sd(breaches$Individuals_Affected, na.rm = TRUE)
stdev

innerQ <-  IQR(breaches$Individuals_Affected, na.rm = TRUE)
innerQ

firstQ <- quantile(breaches$Individuals_Affected, 0.25, na.rm = TRUE)
firstQ <- firstQ[[1]]

thirdQ <- quantile(breaches$Individuals_Affected, 0.75, na.rm = TRUE)
thirdQ <- thirdQ[[1]]

outlier_list <- breaches %>%
  filter(Individuals_Affected < (firstQ - innerQ * 1.5) | 
        Individuals_Affected > (thirdQ + innerQ * 1.5))

outlier_list
```

**- Show how do your distributions look like with and without the unusual values.**


```{r}
outliers_removed <- breaches %>%
  filter(!Individuals_Affected %in% outlier_list$Individuals_Affected) %>%
  ggplot(aes(x=Individuals_Affected))+
  geom_histogram() +
  labs(title = "Outliers Removed")

outliers_included <- breaches %>%
  ggplot(aes(x=Individuals_Affected)) +
  geom_histogram()+
  labs(title = "Outliers Included")

ggarrange(outliers_removed, outliers_included, nrow = 2)
```

**- Discuss whether or not you need to remove unusual values and why.**

The unusual values should not be removed because since the high indivuals affected values will most likely give the most insight into cyber security issues.  

#### 3.1.2.3 Missing values (2 points)

**- Does this variable include missing values? Demonstrate how you determine that.**

No there are no missing values.  The method is.na with the column name can be used and then the vector returned can be turned into a data frame that represents the number of NA values (TRUE) and non NA values (FALSE).  It can also be confirmed by calling summary() on the Individuals Affected variable, which also shows that there are no NA values in the Individuals affected variable.  

```{r}
missing <- is.na(breaches$Individuals_Affected)

num_missing <- as.data.frame(table(missing))

num_missing
  
summary(breaches$Individuals_Affected)
```

**- Demonstrate and discuss how you handle the missing values. E.g., removing, replacing with a constant value, or a value based on the distribution, etc.**

There are no missing values

**- Show how your data looks in each case after handling missing values.Describe and discuss the distribution.** 

There are no missing values.  Refer to histogram and boxplots above for distribution.   
 

#### 3.1.2.4 Does converting the type of this variable help exploring the distribution of its values or identifying outliers or missing values? (3)

Yes converting Individuals affected to a logical may be helpful in exploring the distribution of its values or identifying outliers or missing values since logical are simpler to evaluate when larger continuous data is converted into two groups.  

**- What type can the variable be converted to?**

Individuals affected is of type integer, but it can converted to a logical.  By making the value of Individuals affected TRUE when the value is greater than 20,000 and FALSE when the value is lower than than 20,000, we can see if the Inidviduals Affected level is considered high or not.  Converting Individuals Affected to a logical is a simpler way to interpret Individuals values. The converted Individuals Affected type is saved as a new variable Large_Affected.

```{r}
#function to determine if the ozone levels are healthy using values given in graphic above
high_check <- function(x) {
  if(is.na(x)){
    return(NA)
  }
  else if(x >= 20000){
    return(TRUE)
  }
  else{
    return(FALSE)
  }
}

breaches$large_affected <- sapply(breaches$Individuals_Affected, high_check)
head(breaches)
```


**- How will the distribution look? Please demonstrate with appropriate plots.**

From plotting the converted logical Solar variable as a bar graph, we can see that the majority of the days were low solar days. Logically this makes sense because New York is not known for being incredibly sunny.  We can also see that there are no NA values, which confirms the analysis done earlier. 

```{r}
breaches %>%
  ggplot(aes(x=large_affected, fill = large_affected)) +
  geom_bar()
```

#### 3.1.2.5 What new variables do you need to create?  (3)


**- List the variables**
The new variable large_affected was created above and also explained above.

**- Describe and discuss why they are needed and how you plan to use them.**
Large_affected is needed to look at the outliers of the individuals affected and see if there is a trend with the large values and the states.  I plan to use the logical and see if there is a correlation with the state the breach occured in. 

